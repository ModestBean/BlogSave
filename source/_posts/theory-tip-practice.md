---
title: 【深度理论】神经网络-33条神经网络训练技巧
date: 2019-05-10 11:38:45
tags:
categories: 深度学习
---

此篇文章是我个人对神经网络训练技巧的总结，其参考了公众号量子位在微信上发布的文章。原链接已经给出。想看原文的直接跳转。原文章中总结比我个人总结的好很多，许多话都很通顺，简洁。 [点击链接](https://mp.weixin.qq.com/s/rbCcNGrULYpnVWss4nSI4A) 

此篇文章我加入了我个人所遇到的一些情况，当然也有搬运原文章的内容，因为原文章总结的太好了。

<!--more-->
## 1、先别着急写代码
先别着急一上来就写代码，花多点时间在了解数据集上面。在数据中可能出现重复的样本，或者图像label标记错误等情况。

神经网络实际是数据集的压缩版本，如果网络的预测结果与你的数据中看到的内容不一致，那么就是数据出现了错误。

然后通过搜索，过滤，排序等方式找到影响神经网络的数据，也就是异常值。

我个人遇到的情况：

<img width=1000 src="theory-tip-practice/1.png" >

在编程时我个人遇到的情况，在送入网络时，数据错误。我通过定位数据索引，发现数据中存在很多空值，这里就是说的异常值。应该多花点时间在预处理数据上面。

##  2、设置端到端的训练评估框架
处理完数据集，接下来就能开始训练模型了吗？并不能！下一步是建立一个完整的训练+评估框架。

这个阶段，我们选择一个简单又不至于搞砸的模型，比如线性分类器、CNN，可视化损失。获得准确度等衡量模型的标准，用模型进行预测。

此阶段有这些技巧：（个人体会：这些技巧都是一步步由浅到深的，可以按照顺序来尝试）

 1. 固定随机种子，random.seed()。来保证两次代码获得相同的结果，消除差异因素。
 2. 简单化：在开始训练神经网络的时候先不要扩充数据，扩充数据在后面会用到，在最开始先不要使用。
 3. 在评估中添加有效数字：在绘制测试集损失时，对整个测试集进行评估，不要只绘制批次测试损失图像，然后用Tensorboard对它们进行平滑处理。
 4. 在初始阶段验证损失函数：验证函数是否从正确的损失值开始。例如，如果正确初始化最后一层，则应在softmax初始化时测量-log(1/n_classes)。
 5. 初始化：正确初始化最后一层的权重。如果回归一些平均值为50的值，则将最终偏差初始化为50。如果有一个比例为1:10的不平衡数据集，请设置对数的偏差，使网络预测概率在初始化时为0.1。正确设置这些可以加速模型的收敛。
 6. 人类基线：监控除人为可解释和可检查的损失之外的指标。尽可能评估人的准确性并与之进行比较。或者对测试数据进行两次注释，并且对于每个示例，将一个注释视为预测，将第二个注释视为事实。
 7. 设置一个独立于输入的基线：最简单的方法是将所有输入设置为零，看看模型是否学会从输入中提取任何信息。
 8. 过拟合一个batch：增加了模型的容量并验证我们可以达到的最低损失。
 9. 验证减少训练损失：开始尝试增加数据量。
 10. 在训练模型前进行数据可视化：将原始张量的数据和标签可视化，这样可以减少次数，也就是说可以将shape打印出来，可以很好的来理解数据维度。
 11. 可视化预测动态：在训练过程中对固定测试批次上的模型预测进行可视化。
 12. 使用BP来获得依赖关系：
一个方法是将第i个样本的损失设置为1.0，运行反向传播一直到输入，并确保仅在第i个样本上有非零的梯度。

· 概括一个特例：对正在做的事情编写一个非常具体的函数，让它运行，然后在以后过程中确保能得到相同的结果。

##  3、过拟合的处理方式（这个在Andrew Ng的课程上有很详细的说明，大家可以去参考）
过拟合的技巧：
 1. 预选模型：为了获得较好的训练损失，我们需要为数据选择合适的架构，不要总想着一步到位。如果要做图像分类，只需复制粘贴ResNet-50，我们可以在稍后的过程中做一些自定义的事。  在其基础上可以在进行自己的调整。这就需要我们对常见的神经网络需要有基本的认识。
 2. Adam方法是安全的：在设定基线的早期阶段，使用学习率为3e-4的Adam 。根据经验，亚当对超参数更加宽容，包括不良的学习率
 3. 一次只复杂化一个：如果多个信号输入分类器，建议逐个输入，然后增加复杂性，确保预期的性能逐步提升，而不要一股脑儿全放进去。比如，尝试先插入较小的图像，然后再将它们放大。   也就是先简单到复杂，不要想着一下子吃大
 4. 不要相信学习率衰减默认值：如果不小心，代码可能会过早地将学习率减少到零，导致模型无法收敛。我们完全禁用学习率衰减避免这种状况的发生。
## 正则化（这个在Andrew Ng的课程上也有详细的说明，大家也可以去参考总结）
理想的化，现在有了更大的模型，在训练集上拟合好了，现在该正则化了，舍弃一点训练集上面的准确率，可以换取验证集上的准确率。

这里有一些技巧：

 1. 获取更多数据：最喜欢的正则化的方法，就是添加一些真实的数据。不要在一个小数据集花太大功夫，试图搞出大事情来。有精力去多收集点数据，这是唯一一个确保性能单调提升的方法。
 2. 数据扩增：把数据集做大，除了继续收集数据之外，就是扩增了。旋转，翻转，拉伸，做扩增的时候可以野性一点。
 3. 有创意的扩增：还有什么办法扩增数据集？比如域随机化 (Domain Randomization) ，模拟 (Simulation) ，巧妙的混合 (Hybrids) ，比如把数据插进场景里去。甚至可以用上GAN。
 4. 输入地维一点：把那些可能包含虚假信号的特征去掉，因为这些东西很可能造成过拟合，尤其是数据集不大的时候。
同理，如果低层细节不是那么重要的话，就输入小一点的图片，捕捉高层信息就好了。
 5. 模型小一点：许多情况下，都可以给网络加上领域知识限制 (Domain Knowledge Constraints) ，来把模型变小。
比如，以前很流行在ImageNet的骨架上放全连接层，但现在这种操作已经被平均池化取代了，大大减少了参数。
 6. 减小批尺寸：对批量归一化 (Batch Normalization) 这项操作来说，小批量可能带来更好的正则化效果 (Regularization) 。
 7. Dropout：这个方法不在详细介绍了，不理解Dropout的可以去查看相关文章。
 8. 权重衰减：增加权重衰减的惩罚力度。
 9. 早停法：不用一直训练，可以观察验证集的损失，在要快过拟合的时候，及时喊停。
 10. 试试大一点的模型：我发现，大模型很容易过拟合，几乎是必然，但早停的话，模型可以表现很好。
最后的最后，如果想要更加确信，自己训练出的网络，是个不错的分类器，就把第一层的权重可视化一下，看看边缘 (Edges) 美不美。
如果第一层的过滤器看起来像噪音，就需要再搞一搞了。同理，激活 (Activations) 有时候也会看出瑕疵来，那样就要研究一下哪里出了问题。

## 5、调参
1. 随机网络搜索：（
在同时调整多个超参数的情况下，网格搜索听起来是很诱人，可以把各种设定都包含进来。
但是要记住，随机搜索才是最好的。
直觉上说，这是因为网络通常对其中一些参数比较敏感，对其他参数不那么敏感。
如果参数a是有用的，参数b起不了什么作用，就应该对a取样更彻底一些，不要只在几个固定点上多次取样。
2. 超参数优化：世界上，有许多许多靓丽的贝叶斯超参数优化工具箱，很多小伙伴也给了这些工具好评

## 6、最后的技巧
1. 模型合体：把几个模型结合在一起，至少可以保证提升2%的准确度，不管是什么任务。
如果，你买不起太多的算力，就用蒸馏 (Distill) 把模型们集合成一个神经网络。
2. 放那让它训练吧：通常，人类一看到损失趋于平稳，就停止训练了。
但我感觉，还是训练得昏天黑地，不知道多久了，比较好。
有一次，我意外把一个模型留在那训练了一整个寒假。
我回来的时候，它就成了State-of-the-Art。